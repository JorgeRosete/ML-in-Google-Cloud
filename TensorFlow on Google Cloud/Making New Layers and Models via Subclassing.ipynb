{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2d97c7e31aa"
   },
   "source": [
    "# Making New Layers and Models via Subclassing\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "* Use Layer class as the combination of state (weights) and computation.\n",
    "* Defer weight creation until the shape of the inputs is known.\n",
    "* Build recursively composable layers.\n",
    "* Compute loss using add_loss() method.\n",
    "* Compute average using add_metric() method.\n",
    "* Enable serialization on layers.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial shows how to build new layers and models via [subclassing](https://towardsdatascience.com/model-sub-classing-and-custom-training-loop-from-scratch-in-tensorflow-2-cc1d4f10fb4e).\n",
    "__Subclassing__ is a term that refers inheriting properties for a new object from a base or superclass object.\n",
    "\n",
    "Each learning objective will correspond to a __#TODO__ in the [student lab notebook](../labs/custom_layers_and_models.ipynb) -- try to complete that notebook first before reviewing this solution notebook. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8d4ac441b1fc"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4e7dce39dd1d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7b363673d96c"
   },
   "source": [
    "## The `Layer` class: the combination of state (weights) and some computation\n",
    "\n",
    "One of the central abstraction in Keras is the `Layer` class. A layer\n",
    "encapsulates both a state (the layer's \"weights\") and a transformation from\n",
    "inputs to outputs (a \"call\", the layer's forward pass).\n",
    "\n",
    "Here's a densely-connected layer. It has a state: the variables `w` and `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "59b8317dbd3c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a Linear class\n",
    "# A class called 'Linear' is defined which inherits from 'keras.layers.Layer'.\n",
    "# This means that 'Linear' is a custom layer type that can be used in a Keras model.\n",
    "class Linear(keras.layers.Layer):\n",
    "    # The '__init__' method is the constructor of the class and is called when an instance of 'Linear' is created.\n",
    "    # 'units' specifies the number of units (neurons) in the layer.\n",
    "    # 'input_dim' specifies the input dimension.\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        w_init = tf.random_normal_initializer() # 'w_init' is a weight initializer that initializes weights with random values according to a normal distribution.\n",
    "        # 'self.w' is a TensorFlow variable representing the layer weights.\n",
    "        # It is initialized with an array of size '(input_dim, units)' and type 'float32'.\n",
    "        # It is a trainable variable, which means that it will be updated during model training.\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=w_init(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "        # b_init is a bias initializer that initializes the biases with zeros.\n",
    "        # self.b is a TensorFlow variable that represents the layer biases.\n",
    "        # It is initialized with a vector of size (units,) and type float32. It is also a trainable variable.\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(\n",
    "            initial_value=b_init(shape=(units,), dtype=\"float32\"), trainable=True\n",
    "        )\n",
    "\n",
    "    # The call method is defined to specify the calculation logic of the layer.\n",
    "    # inputs are the inputs to the layer.\n",
    "    # tf.matmul(inputs, self.w) performs a matrix multiplication between inputs and weights.\n",
    "    # + self.b adds the bias to each output.\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dac8fb03a642"
   },
   "source": [
    "You would use a layer by calling it on some tensor input(s), much like a Python\n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cdcd15d5e68a",
    "outputId": "b2178b8c-c564-4e17-a634-5bd2f79cc0fc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.0819068  -0.12334651 -0.05264889  0.00625704]\n",
      " [ 0.0819068  -0.12334651 -0.05264889  0.00625704]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# A TensorFlow tensor 'x' is created in the form (2, 2), which means that it is a matrix of 2 rows and 2 columns.\n",
    "x = tf.ones((2, 2))\n",
    "\n",
    "# An instance of the 'Linear' class is created with 'units=4' and 'input_dim=2'.\n",
    "# This means that the layer will have 4 neurons and expects inputs of dimension 2.\n",
    "linear_layer = Linear(4, 2)\n",
    "\n",
    "# 'y' will be the result of the linear layer operation applied on 'x'.\n",
    "y = linear_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "382960020a56"
   },
   "source": [
    "Note that the weights `w` and `b` are automatically tracked by the layer upon\n",
    "being set as layer attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "d3d875af9465",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the 'assert' statement to check if 'linear_layer.weights' is equal to '[linear_layer.w, linear_layer.b]'.\n",
    "# This ensures that the 'weights' variables of the 'linear_layer' layer exactly match the 'w' (weights) and 'b' (biases) variables\n",
    "# we have defined within the 'Linear' class.\n",
    "assert linear_layer.weights == [linear_layer.w, linear_layer.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec9d72aa7538"
   },
   "source": [
    "Note you also have access to a quicker shortcut for adding weight to a layer:\n",
    "the `add_weight()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "168548eba841",
    "outputId": "e74521a0-228e-49c5-80ec-7aa6b4c49003",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.04942621  0.02780763 -0.10388044 -0.02482288]\n",
      " [-0.04942621  0.02780763 -0.10388044 -0.02482288]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Use `add_weight()` method for adding weight to a layer\n",
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__() # calls the constructor of the base class 'keras.layers.Layer'\n",
    "        # is used to add weight variables (w and b) to the layer\n",
    "        # represents the weights and are randomly initialized according to a normal distribution using the initializer 'random_normal'\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_dim, units), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "        # represents the biases and are initialized with zeros using the initializer 'zeros'.\n",
    "        self.b = self.add_weight(shape=(units,), initializer=\"zeros\", trainable=True)\n",
    "\n",
    "    # Defines the operation of the layer during forward propagation.\n",
    "    def call(self, inputs):\n",
    "        # inputs are the inputs to the layer.\n",
    "        # 'tf.matmul(inputs, self.w)' performs the matrix multiplication of the inputs by the weights.\n",
    "        # '+ self.b' adds the biases to the output of the matrix multiplication.\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "# Create an input tensor 'x' with all elements initialized to one and of form '(2, 2)'.\n",
    "x = tf.ones((2, 2))\n",
    "# Instantiate a 'linear_layer' object of class 'Linear' with 'units=4' and 'input_dim=2'\n",
    "linear_layer = Linear(4, 2)\n",
    "# The 'call' method of the 'linear_layer' object with 'x' as input, which applies the linear layer on 'x'.\n",
    "y = linear_layer(x)\n",
    "# The result 'y' is the output of the linear layer after applying it on 'x'.\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "070ea9b4db6c"
   },
   "source": [
    "## Layers can have non-trainable weights\n",
    "\n",
    "Besides trainable weights, you can add non-trainable weights to a layer as\n",
    "well. Such weights are meant not to be taken into account during\n",
    "backpropagation, when you are training the layer.\n",
    "\n",
    "Here's how to add and use a non-trainable weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7c4cb404145f",
    "outputId": "8925cf86-3d5e-4f88-dfc5-b6c8866d3122",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2.]\n",
      "[4. 4.]\n"
     ]
    }
   ],
   "source": [
    "# Add and use a non-trainable weight\n",
    "class ComputeSum(keras.layers.Layer):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ComputeSum, self).__init__() # calls the constructor of the base class 'keras.layers.Layer'\n",
    "\n",
    "        # 'tf.Variable(initial_value=tf.zeros((input_dim,)), trainable=False)' initializes self.total with a tensor of zeros of form '(input_dim,)'\n",
    "        self.total = tf.Variable(initial_value=tf.zeros((input_dim,)), trainable=False) # TensorFlow variable representing the cumulative sum.\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # 'tf.reduce_sum(inputs, axis=0)' calculates the sum along axis 0 (rows) of the inputs.\n",
    "        # 'self.total.assign_add(tf.reduce_sum(inputs, axis=0))' adds the calculated sum to self.total.\n",
    "        self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
    "        return self.total\n",
    "\n",
    "# An input tensor 'x' is created with all elements initialized to one and of form '(2, 2)'.\n",
    "x = tf.ones((2, 2))\n",
    "\n",
    "# Instantiate an object 'my_sum' of class 'ComputeSum' with 'input_dim=2'\n",
    "my_sum = ComputeSum(2)\n",
    "\n",
    "# Call the 'call' method of the 'my_sum' object with 'x' as input, which adds the sum of 'x' to 'my_sum.total' and returns the accumulated value\n",
    "y = my_sum(x)\n",
    "\n",
    "# 'print(y.numpy())' prints the accumulated value after the first call\n",
    "print(y.numpy())\n",
    "\n",
    "# A second call is made to 'my_sum(x)', which adds the sum of 'x' to 'my_sum.total' again\n",
    "y = my_sum(x)\n",
    "\n",
    "# 'print(y.numpy())' prints the accumulated value after the second call\n",
    "print(y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40f5b74d3d87"
   },
   "source": [
    "It's part of `layer.weights`, but it gets categorized as a non-trainable weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3d4db4ef4fa4",
    "outputId": "b6d4febb-b612-483c-8d7e-485931a7cdad",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: 1\n",
      "non-trainable weights: 1\n",
      "trainable_weights: []\n"
     ]
    }
   ],
   "source": [
    "print(\"weights:\", len(my_sum.weights)) # returns a list of all weight variables within the layer 'my_sum'.\n",
    "\n",
    "# 'my_sum.non_trainable_weights' returns a list of all weight variables that are not trainable within the 'my_sum' layer.\n",
    "print(\"non-trainable weights:\", len(my_sum.non_trainable_weights)) # prints the number of weight variables in 'my_sum'.\n",
    "\n",
    "# It's not included in the trainable weights:\n",
    "print(\"trainable_weights:\", my_sum.trainable_weights) # returns a list of all weight variables that are trainable within the layer 'my_sum'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fe6942aff7c6"
   },
   "source": [
    "## Best practice: deferring weight creation until the shape of the inputs is known\n",
    "\n",
    "Our `Linear` layer above took an `input_dim` argument that was used to compute\n",
    "the shape of the weights `w` and `b` in `__init__()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "275b68d5ea9f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This code defines a custom linear layer Linear using the TensorFlow/Keras functional API.\n",
    "class Linear(keras.layers.Layer): # This is the constructor of the 'Linear' class.\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__() # 'super(Linear, self).__init__()' calls the constructor of the base class 'keras.layers.Layer'.\n",
    "        \n",
    "        # 'self.w' represents the weights and is initialized randomly using the 'random_normal' initializer.\n",
    "        self.w = self.add_weight( # 'self.add_weight()' is used to add weight variables ('w' and 'b') to the layer.\n",
    "            shape=(input_dim, units), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "        # 'self.b' represents the biases and is initialized with zeros using the 'zeros' initializer.\n",
    "        # Both variables ('self.w' and 'self.b') are marked as trainable ('trainable=True')\n",
    "        self.b = self.add_weight(shape=(units,), initializer=\"zeros\", trainable=True)\n",
    "\n",
    "    # Defines the operation of the layer during forward propagation.\n",
    "    def call(self, inputs): # 'inputs' are the inputs to the layer.\n",
    "        \n",
    "        # 'tf.matmul(inputs, self.w)' performs matrix multiplication of the inputs by the weights.\n",
    "        return tf.matmul(inputs, self.w) + self.b # '+ self.b' adds the biases to the output of the matrix multiplication.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ebcacebb348"
   },
   "source": [
    "In many cases, you may not know in advance the size of your inputs, and you\n",
    "would like to lazily create weights when that value becomes known, some time\n",
    "after instantiating the layer.\n",
    "\n",
    "In the Keras API, we recommend creating layer weights in the `build(self, input_shape)` method of your layer. Like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "118c899f427e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "class Linear(keras.layers.Layer):\n",
    "    # Initializes the Linear class with a parameter units, which specifies the number of output units (neurons) for the layer\n",
    "    def __init__(self, units=32):\n",
    "        super(Linear, self).__init__() # calls the constructor of the base class 'keras.layers.Layer'\n",
    "        self.units = units # stores the number of output units for the layer\n",
    "\n",
    "    # This method is called when the layer is built, typically when it is first used or when 'model.build()' is called\n",
    "    def build(self, input_shape): # is the shape of the input tensor that the layer will receive during execution\n",
    "        self.w = self.add_weight( # is used to create trainable variables ('w' for weights and 'b' for biases) for the layer.\n",
    "            shape=(input_shape[-1], self.units), # specifies the shape of 'self.w' based on the last dimension of the input shape and 'self.units'\n",
    "            initializer=\"random_normal\", # initializes both 'self.w' and 'self.b' with random values from a normal distribution.\n",
    "            trainable=True, # marks both 'self.w' and 'self.b' as trainable variables\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "\n",
    "    # Defines the computation performed by the layer during forward pass (inference)\n",
    "    def call(self, inputs):\n",
    "        # 'tf.matmul(inputs, self.w)' computes the matrix multiplication of the inputs and 'self.w'\n",
    "        return tf.matmul(inputs, self.w) + self.b # '+ self.b' adds the biases 'self.b' to the result of the matrix multiplication\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78061e0583c6"
   },
   "source": [
    "The `__call__()` method of your layer will automatically run build the first time\n",
    "it is called. You now have a layer that's lazy and thus easier to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "fb08b1a45d22",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# At instantiation, we don't know on what inputs this is going to get called\n",
    "linear_layer = Linear(32)\n",
    "\n",
    "# The layer's weights are created dynamically the first time the layer is called\n",
    "y = linear_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddd7e8b22641"
   },
   "source": [
    "## Layers are recursively composable\n",
    "\n",
    "If you assign a Layer instance as an attribute of another Layer, the outer layer\n",
    "will start tracking the weights of the inner layer.\n",
    "\n",
    "We recommend creating such sublayers in the `__init__()` method (since the\n",
    "sublayers will typically have a build method, they will be built when the\n",
    "outer layer gets built)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9561cbf2fc60",
    "outputId": "5ccf20ea-f734-48c2-ac20-c16a4eb54fba",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: 6\n",
      "trainable weights: 6\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# Let's assume we are reusing the Linear class\n",
    "# with a `build` method that we defined above.\n",
    "\n",
    "\n",
    "class MLPBlock(keras.layers.Layer):\n",
    "    # Initializes the 'MLPBlock' class by calling the constructor of the base class 'keras.layers.Layer'\n",
    "    def __init__(self):\n",
    "        super(MLPBlock, self).__init__()\n",
    "        self.linear_1 = Linear(32) # self.linear_1 with 32 output units.\n",
    "        self.linear_2 = Linear(32) # self.linear_2 with 32 output units.\n",
    "        self.linear_3 = Linear(1) # self.linear_3 with 1 output units.\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.linear_1(inputs) # Passes the inputs through self.linear_1, followed by applying the ReLU activation function\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.linear_2(x) # Passes the result through self.linear_2, followed by applying the ReLU activation function\n",
    "        x = tf.nn.relu(x)\n",
    "        return self.linear_3(x) # Passes the result through self.linear_3 and returns the output\n",
    "\n",
    "\n",
    "mlp = MLPBlock()\n",
    "# Calls the instance with an input tensor of shape (3, 64), which triggers the forward pass and initializes the weights for the first time\n",
    "y = mlp(tf.ones(shape=(3, 64)))  # The first call to the `mlp` will create the weights\n",
    "\n",
    "# 'mlp.weights' returns a list of all weights in the 'mlp' block.\n",
    "# 'len(mlp.weights)' prints the number of weights in the 'mlp' block.\n",
    "print(\"weights:\", len(mlp.weights))\n",
    "\n",
    "# 'mlp.trainable_weights' returns a list of all trainable weights in the 'mlp' block.\n",
    "# 'len(mlp.trainable_weights)' prints the number of trainable weights in the 'mlp' block\n",
    "print(\"trainable weights:\", len(mlp.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "496736d98a62"
   },
   "source": [
    "## The `add_loss()` method\n",
    "\n",
    "When writing the `call()` method of a layer, you can create loss tensors that\n",
    "you will want to use later, when writing your training loop. This is doable by\n",
    "calling `self.add_loss(value)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "084d56602ca4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A layer that creates an activity regularization loss\n",
    "class ActivityRegularizationLayer(keras.layers.Layer):\n",
    "    def __init__(self, rate=1e-2): # Initializes the 'ActivityRegularizationLayer' class with a parameter 'rate', which specifies the regularization rate\n",
    "        # 'super(ActivityRegularizationLayer, self).__init__()' calls the constructor of the base class 'keras.layers.Layer'\n",
    "        super(ActivityRegularizationLayer, self).__init__()\n",
    "        self.rate = rate # 'self.rate' stores the regularization rate, defaulting to '1e-2'\n",
    "\n",
    "    # Defines the computation performed by the layer during forward pass (inference)\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        self.add_loss(self.rate * tf.reduce_sum(inputs)) # adds the computed regularization loss term to the overall loss of the model\n",
    "        return inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5009ff0d1feb"
   },
   "source": [
    "These losses (including those created by any inner layer) can be retrieved via\n",
    "`layer.losses`. This property is reset at the start of every `__call__()` to\n",
    "the top-level layer, so that `layer.losses` always contains the loss values\n",
    "created during the last forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "5bff6d59aea7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "class OuterLayer(keras.layers.Layer): # Initializes the OuterLayer class\n",
    "    def __init__(self):\n",
    "        super(OuterLayer, self).__init__() # Calls the constructor of the base class 'keras.layers.Layer'\n",
    "        self.activity_reg = ActivityRegularizationLayer(1e-2) # Creates an instance of 'ActivityRegularizationLayer' with a regularization rate of '1e-2'\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # The method simply forwards the inputs to 'self.activity_reg' and returns the output of the 'ActivityRegularizationLayer'\n",
    "        return self.activity_reg(inputs)\n",
    "\n",
    "\n",
    "layer = OuterLayer()\n",
    "\n",
    "# Asserts that 'layer.losses' is empty because the layer has not been called yet, so no losses have been added\n",
    "assert len(layer.losses) == 0  # No losses yet since the layer has never been called\n",
    "\n",
    "_ = layer(tf.zeros(1, 1))\n",
    "assert len(layer.losses) == 1  # We created one loss value\n",
    "\n",
    "# `layer.losses` gets reset at the start of each __call__\n",
    "_ = layer(tf.zeros(1, 1))\n",
    "assert len(layer.losses) == 1  # This is the loss created during the call above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36751ebe3363"
   },
   "source": [
    "In addition, the `loss` property also contains regularization losses created\n",
    "for the weights of any inner layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9327d3b581f8",
    "outputId": "b072d503-c9e1-40b1-ab15-e79416c9c958",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=0.00266289>]\n"
     ]
    }
   ],
   "source": [
    "class OuterLayerWithKernelRegularizer(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        # 'super(OuterLayerWithKernelRegularizer, self).__init__()' calls the constructor of the base class 'keras.layers.Layer'\n",
    "        super(OuterLayerWithKernelRegularizer, self).__init__()\n",
    "        self.dense = keras.layers.Dense(\n",
    "            # self.dense creates an instance of a Dense layer with 32 output units and an L2 kernel regularizer with a regularization rate of 1e-3\n",
    "            32, kernel_regularizer=tf.keras.regularizers.l2(1e-3)\n",
    "        )\n",
    "\n",
    "    # Defines the computation performed by the layer during the forward pass\n",
    "    def call(self, inputs): # 'inputs' are the input tensors to the layer\n",
    "        return self.dense(inputs) # The method simply forwards the inputs to 'self.dense' and returns the output of the 'Dense' layer\n",
    "\n",
    "\n",
    "layer = OuterLayerWithKernelRegularizer() # Creates an instance of 'OuterLayerWithKernelRegularizer'\n",
    "_ = layer(tf.zeros((1, 1))) # Calls the 'layer' with a tensor of zeros with shape '(1, 1)', which triggers the forward pass\n",
    "\n",
    "# This is `1e-3 * sum(layer.dense.kernel ** 2)`,\n",
    "# created by the `kernel_regularizer` above.\n",
    "print(layer.losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99d502b8899c"
   },
   "source": [
    "These losses are meant to be taken into account when writing training loops,\n",
    "like this:\n",
    "\n",
    "```python\n",
    "# Instantiate an optimizer.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Iterate over the batches of a dataset.\n",
    "for x_batch_train, y_batch_train in train_dataset:\n",
    "  with tf.GradientTape() as tape:\n",
    "    logits = layer(x_batch_train)  # Logits for this minibatch\n",
    "    # Loss value for this minibatch\n",
    "    loss_value = loss_fn(y_batch_train, logits)\n",
    "    # Add extra losses created during this forward pass:\n",
    "    loss_value += sum(model.losses)\n",
    "\n",
    "  grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fa2db4a631d"
   },
   "source": [
    "For a detailed guide about writing training loops, see the\n",
    "[guide to writing a training loop from scratch](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch/).\n",
    "\n",
    "These losses also work seamlessly with `fit()` (they get automatically summed\n",
    "and added to the main loss, if any):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5c767ccdfc43",
    "outputId": "97c613c4-f5f7-4ddf-f2d1-6d63b7eba791",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-16 21:48:25.515679: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 153ms/step - loss: 0.1295\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0261\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe7b4b16070>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = keras.Input(shape=(3,)) # Define the input layer with a shape of '(3,)', meaning each input sample has 3 features\n",
    "outputs = ActivityRegularizationLayer()(inputs) # Pass the inputs through the 'ActivityRegularizationLayer'\n",
    "model = keras.Model(inputs, outputs) # Create a Keras model that maps the 'inputs' to the outputs\n",
    "\n",
    "# If there is a loss passed in `compile`, the regularization\n",
    "# losses get added to it\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\") # The model is compiled with the Adam optimizer and mean squared error (MSE) loss function\n",
    "model.fit(np.random.random((2, 3)), np.random.random((2, 3)))\n",
    "\n",
    "# It's also possible not to pass any loss in `compile`,\n",
    "# since the model already has a loss to minimize, via the `add_loss`\n",
    "# call during the forward pass!\n",
    "model.compile(optimizer=\"adam\") # The model is compiled again with the Adam optimizer. This time, no loss function is specified, so the default loss function (if any) is used\n",
    "model.fit(np.random.random((2, 3)), np.random.random((2, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c753fcbc1818"
   },
   "source": [
    "## The `add_metric()` method\n",
    "\n",
    "Similarly to `add_loss()`, layers also have an `add_metric()` method\n",
    "for tracking the moving average of a quantity during training.\n",
    "\n",
    "Consider the following layer: a \"logistic endpoint\" layer.\n",
    "It takes as inputs predictions & targets, it computes a loss which it tracks\n",
    "via `add_loss()`, and it computes an accuracy scalar, which it tracks via\n",
    "`add_metric()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "85dad61dc160",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This code defines a custom Keras layer LogisticEndpoint that computes both the loss and accuracy for a binary classification problem.\n",
    "# This layer can be integrated into a model to handle the loss and accuracy calculations.\n",
    "class LogisticEndpoint(keras.layers.Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super(LogisticEndpoint, self).__init__(name=name)\n",
    "\n",
    "        # Initializes 'self.loss_fn' as an instance of 'BinaryCrossentropy' with 'from_logits=True',\n",
    "        # indicating that the logits (raw predictions) will be provided directly\n",
    "        self.loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "        # Initializes 'self.accuracy_fn' as an instance of 'BinaryAccuracy', a metric to compute the accuracy of binary classification predictions\n",
    "        self.accuracy_fn = keras.metrics.BinaryAccuracy()\n",
    "\n",
    "    # 'targets': The true labels for the data,\n",
    "    # 'logits': The raw predictions (logits) from the model,\n",
    "    # 'sample_weights': Optional weights for each sample, default is 'None'\n",
    "    def call(self, targets, logits, sample_weights=None):\n",
    "        # Compute the training-time loss value and add it\n",
    "        # to the layer using `self.add_loss()`.\n",
    "        loss = self.loss_fn(targets, logits, sample_weights) # Computes the binary cross-entropy loss using 'self.loss_fn'\n",
    "        self.add_loss(loss) # Adds this loss to the model's loss using 'self.add_loss(loss)'\n",
    "\n",
    "        # Log accuracy as a metric and add it\n",
    "        # to the layer using `self.add_metric()`.\n",
    "        acc = self.accuracy_fn(targets, logits, sample_weights)\n",
    "        self.add_metric(acc, name=\"accuracy\")\n",
    "\n",
    "        # Return the inference-time prediction tensor (for `.predict()`).\n",
    "        return tf.nn.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cd8807cb9cbc"
   },
   "source": [
    "Metrics tracked in this way are accessible via `layer.metrics`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "475df7270265",
    "outputId": "068c596b-5cb1-457b-dde3-63237538fdd1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer.metrics: [<keras.metrics.BinaryAccuracy object at 0x7fe7b47826d0>]\n",
      "current accuracy value: 1.0\n"
     ]
    }
   ],
   "source": [
    "# This instance will be used to compute the binary cross-entropy loss and accuracy for given targets and logits\n",
    "layer = LogisticEndpoint()\n",
    "\n",
    "targets = tf.ones((2, 2)) # These represent the true labels for two samples, each with two target values.\n",
    "logits = tf.ones((2, 2)) # These represent the raw predictions from the model\n",
    "\n",
    "#  The call method of LogisticEndpoint computes the loss and accuracy, adds them to the model's losses and metrics, and returns the softmax of the logits\n",
    "y = layer(targets, logits)\n",
    "\n",
    "print(\"layer.metrics:\", layer.metrics)\n",
    "print(\"current accuracy value:\", float(layer.metrics[0].result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eda5113fd18"
   },
   "source": [
    "Just like for `add_loss()`, these metrics are tracked by `fit()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "97f767613953",
    "outputId": "d6877e1f-bae4-4979-dd65-9b013d66c6af",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 364ms/step - loss: 0.8632 - binary_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe7b4afdc40>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This model takes two inputs: the feature inputs and the targets\n",
    "\n",
    "inputs = keras.Input(shape=(3,), name=\"inputs\") # Define an input layer with a shape of '(3,)' for the feature inputs\n",
    "targets = keras.Input(shape=(10,), name=\"targets\") # Define an input layer with a shape of '(10,)' for the target labels\n",
    "logits = keras.layers.Dense(10)(inputs) # Add a dense layer with 10 units. This layer processes the feature inputs\n",
    "# Use the LogisticEndpoint layer to compute predictions and track the loss and accuracy\n",
    "predictions = LogisticEndpoint(name=\"predictions\")(logits, targets)\n",
    "\n",
    "model = keras.Model(inputs=[inputs, targets], outputs=predictions) # Create a Keras model with the specified inputs and outputs\n",
    "model.compile(optimizer=\"adam\") # Compile the model with the Adam optimizer\n",
    "\n",
    "data = { # Generate random data for the inputs and targets. The inputs have a shape of '(3, 3)' and the targets have a shape of '(3, 10)'\n",
    "    \"inputs\": np.random.random((3, 3)),\n",
    "    \"targets\": np.random.random((3, 10)),\n",
    "}\n",
    "model.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bdbac3f6c85"
   },
   "source": [
    "## You can optionally enable serialization on your layers\n",
    "\n",
    "If you need your custom layers to be serializable as part of a\n",
    "[Functional model](https://www.tensorflow.org/guide/keras/functional/), you can optionally implement a `get_config()`\n",
    "method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b359ed5289a8",
    "outputId": "adc17d2d-f5d9-4206-d716-df6185d2f868",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'units': 64}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This code defines a custom Keras layer called 'Linear' and demonstrates how to use the 'get_config and\n",
    "# 'from_config' methods to serialize and deserialize the layer\n",
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "   # This method is called once the shape of the input is known\n",
    "    def build(self, input_shape):\n",
    "        # Defines and initializes the weights 'w' and biases 'b' of the layer using 'add_weight'\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units), # is initialized with a random normal distribution\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs): # Performs the forward pass of the layer\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "    def get_config(self): # Returns a dictionary containing the configuration of the layer\n",
    "        return {\"units\": self.units}\n",
    "\n",
    "\n",
    "# You can enable serialization on your layers using `get_config()` method\n",
    "# Now you can recreate the layer from its config:\n",
    "layer = Linear(64) #  Creates an instance of the 'Linear' layer with 64 units\n",
    "config = layer.get_config() # Calls the 'get_config' method to retrieve the configuration of the layer, which is a dictionary '{\"units\": 64}'\n",
    "print(config)\n",
    "\n",
    "# Creates a new instance of the 'Linear' layer from the configuration dictionary using the 'from_config' method\n",
    "new_layer = Linear.from_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78b207f7acbc"
   },
   "source": [
    "Note that the `__init__()` method of the base `Layer` class takes some keyword\n",
    "arguments, in particular a `name` and a `dtype`. It's good practice to pass\n",
    "these arguments to the parent class in `__init__()` and to include them in the\n",
    "layer config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "00a3432cd28c",
    "outputId": "3a2672d4-b8bd-4fe4-cb00-1e06f3e0f87a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'linear_9', 'trainable': True, 'dtype': 'float32', 'units': 64}\n"
     ]
    }
   ],
   "source": [
    "# This code defines a custom Keras layer called 'Linear' and demonstrates how to use the 'get_config' and 'from_config'\n",
    "# methods to serialize and deserialize the layer, including any additional keyword arguments ('kwargs')\n",
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32, **kwargs): # Initializes the 'Linear' layer with a specified number of units (default is 32)\n",
    "        # Calls the constructor of the base class 'keras.layers.Layer' with any additional keyword arguments ('kwargs')\n",
    "        super(Linear, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Defines and initializes the weights 'w' and biases 'b' of the layer using 'add_weight'\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units), # is initialized with a random normal distribution\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b # Performs the forward pass of the layer\n",
    "\n",
    "    def get_config(self):\n",
    "        # Calls the 'get_config' method of the base class and updates the returned dictionary with the 'units' parameter\n",
    "        config = super(Linear, self).get_config()\n",
    "        config.update({\"units\": self.units})\n",
    "        return config\n",
    "\n",
    "\n",
    "layer = Linear(64) # Creates an instance of the Linear layer with 64 units\n",
    "\n",
    "# Calls the 'get_config' method to retrieve the configuration of the layer,\n",
    "# which is a dictionary including 'units' and any other configurations from the base class\n",
    "config = layer.get_config() # This method is used to return a dictionary containing the configuration of the layer.\n",
    "print(config)\n",
    "new_layer = Linear.from_config(config) # This class method is used to create a new instance of the layer from the configuration dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ad5d100cc969"
   },
   "source": [
    "If you need more flexibility when deserializing the layer from its config, you\n",
    "can also override the `from_config()` class method. This is the base\n",
    "implementation of `from_config()`:\n",
    "\n",
    "```python\n",
    "def from_config(cls, config):\n",
    "  return cls(**config)\n",
    "```\n",
    "\n",
    "To learn more about serialization and saving, see the complete\n",
    "[guide to saving and serializing models](https://www.tensorflow.org/guide/keras/save_and_serialize/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "741c6d134d65"
   },
   "source": [
    "## Privileged `training` argument in the `call()` method\n",
    "\n",
    "Some layers, in particular the `BatchNormalization` layer and the `Dropout`\n",
    "layer, have different behaviors during training and inference. For such\n",
    "layers, it is standard practice to expose a `training` (boolean) argument in\n",
    "the `call()` method.\n",
    "\n",
    "By exposing this argument in `call()`, you enable the built-in training and\n",
    "evaluation loops (e.g. `fit()`) to correctly use the layer in training and\n",
    "inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "67ca741d0cfb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDropout(keras.layers.Layer):\n",
    "    # Calls the constructor of the base class 'keras.layers.Layer' with any additional keyword arguments ('kwargs')\n",
    "    def __init__(self, rate, **kwargs):\n",
    "        super(CustomDropout, self).__init__(**kwargs)\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        \n",
    "        # The 'training' argument indicates whether the layer is in training mode. If 'training' is 'True', \n",
    "        # it applies dropout to the inputs using 'tf.nn.dropout' with the specified rate\n",
    "        if training:\n",
    "            return tf.nn.dropout(inputs, rate=self.rate)\n",
    "        return inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5284e22677da"
   },
   "source": [
    "## Privileged `mask` argument in the `call()` method\n",
    "\n",
    "The other privileged argument supported by `call()` is the `mask` argument.\n",
    "\n",
    "You will find it in all Keras RNN layers. A mask is a boolean tensor (one\n",
    "boolean value per timestep in the input) used to skip certain input timesteps\n",
    "when processing timeseries data.\n",
    "\n",
    "Keras will automatically pass the correct `mask` argument to `__call__()` for\n",
    "layers that support it, when a mask is generated by a prior layer.\n",
    "Mask-generating layers are the `Embedding`\n",
    "layer configured with `mask_zero=True`, and the `Masking` layer.\n",
    "\n",
    "To learn more about masking and how to write masking-enabled layers, please\n",
    "check out the guide\n",
    "[\"understanding padding and masking\"](https://www.tensorflow.org/guide/keras/masking_and_padding/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bf87358118de"
   },
   "source": [
    "## The `Model` class\n",
    "\n",
    "In general, you will use the `Layer` class to define inner computation blocks,\n",
    "and will use the `Model` class to define the outer model -- the object you\n",
    "will train.\n",
    "\n",
    "For instance, in a ResNet50 model, you would have several ResNet blocks\n",
    "subclassing `Layer`, and a single `Model` encompassing the entire ResNet50\n",
    "network.\n",
    "\n",
    "The `Model` class has the same API as `Layer`, with the following differences:\n",
    "\n",
    "- It exposes built-in training, evaluation, and prediction loops\n",
    "(`model.fit()`, `model.evaluate()`, `model.predict()`).\n",
    "- It exposes the list of its inner layers, via the `model.layers` property.\n",
    "- It exposes saving and serialization APIs (`save()`, `save_weights()`...)\n",
    "\n",
    "Effectively, the `Layer` class corresponds to what we refer to in the\n",
    "literature as a \"layer\" (as in \"convolution layer\" or \"recurrent layer\") or as\n",
    "a \"block\" (as in \"ResNet block\" or \"Inception block\").\n",
    "\n",
    "Meanwhile, the `Model` class corresponds to what is referred to in the\n",
    "literature as a \"model\" (as in \"deep learning model\") or as a \"network\" (as in\n",
    "\"deep neural network\").\n",
    "\n",
    "So if you're wondering, \"should I use the `Layer` class or the `Model` class?\",\n",
    "ask yourself: will I need to call `fit()` on it? Will I need to call `save()`\n",
    "on it? If so, go with `Model`. If not (either because your class is just a block\n",
    "in a bigger system, or because you are writing training & saving code yourself),\n",
    "use `Layer`.\n",
    "\n",
    "For instance, we could take our mini-resnet example above, and use it to build\n",
    "a `Model` that we could train with `fit()`, and that we could save with\n",
    "`save_weights()`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cd8cfcb1bb2b"
   },
   "source": [
    "```python\n",
    "class ResNet(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.block_1 = ResNetBlock()\n",
    "        self.block_2 = ResNetBlock()\n",
    "        self.global_pool = layers.GlobalAveragePooling2D()\n",
    "        self.classifier = Dense(num_classes)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.block_1(inputs)\n",
    "        x = self.block_2(x)\n",
    "        x = self.global_pool(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "resnet = ResNet()\n",
    "dataset = ...\n",
    "resnet.fit(dataset, epochs=10)\n",
    "resnet.save(filepath)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b817a4de8c5d"
   },
   "source": [
    "## Putting it all together: an end-to-end example\n",
    "\n",
    "Here's what you've learned so far:\n",
    "\n",
    "- A `Layer` encapsulate a state (created in `__init__()` or `build()`) and some\n",
    "computation (defined in `call()`).\n",
    "- Layers can be recursively nested to create new, bigger computation blocks.\n",
    "- Layers can create and track losses (typically regularization losses) as well\n",
    "as metrics, via `add_loss()` and `add_metric()`\n",
    "- The outer container, the thing you want to train, is a `Model`. A `Model` is\n",
    "just like a `Layer`, but with added training and serialization utilities.\n",
    "\n",
    "Let's put all of these things together into an end-to-end example: we're going\n",
    "to implement a Variational AutoEncoder (VAE). We'll train it on MNIST digits.\n",
    "\n",
    "Our VAE will be a subclass of `Model`, built as a nested composition of layers\n",
    "that subclass `Layer`. It will feature a regularization loss (KL divergence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "18842173f875",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    \"\"\"Maps MNIST digits to a triplet (z_mean, z_log_var, z).\"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim=32, intermediate_dim=64, name=\"encoder\", **kwargs):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation=\"relu\")\n",
    "        self.dense_mean = layers.Dense(latent_dim)\n",
    "        self.dense_log_var = layers.Dense(latent_dim)\n",
    "        self.sampling = Sampling()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        z_mean = self.dense_mean(x)\n",
    "        z_log_var = self.dense_log_var(x)\n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        return z_mean, z_log_var, z\n",
    "\n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    \"\"\"Converts z, the encoded digit vector, back into a readable digit.\"\"\"\n",
    "\n",
    "    def __init__(self, original_dim, intermediate_dim=64, name=\"decoder\", **kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation=\"relu\")\n",
    "        self.dense_output = layers.Dense(original_dim, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        return self.dense_output(x)\n",
    "\n",
    "\n",
    "class VariationalAutoEncoder(keras.Model):\n",
    "    \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_dim,\n",
    "        intermediate_dim=64,\n",
    "        latent_dim=32,\n",
    "        name=\"autoencoder\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)\n",
    "        self.original_dim = original_dim\n",
    "        self.encoder = Encoder(latent_dim=latent_dim, intermediate_dim=intermediate_dim)\n",
    "        self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        # Add KL divergence regularization loss.\n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1\n",
    "        )\n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstructed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40384d934b3c"
   },
   "source": [
    "Let's write a simple training loop on MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c37fef01d4bc",
    "outputId": "045938b8-4048-4fec-87db-aec2b641b084",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "11501568/11490434 [==============================] - 0s 0us/step\n",
      "Start of epoch 0\n",
      "step 0: mean loss = 0.3371\n",
      "step 100: mean loss = 0.1245\n",
      "step 200: mean loss = 0.0986\n",
      "step 300: mean loss = 0.0888\n",
      "step 400: mean loss = 0.0839\n",
      "step 500: mean loss = 0.0807\n",
      "step 600: mean loss = 0.0786\n",
      "step 700: mean loss = 0.0770\n",
      "step 800: mean loss = 0.0758\n",
      "step 900: mean loss = 0.0748\n",
      "Start of epoch 1\n",
      "step 0: mean loss = 0.0745\n",
      "step 100: mean loss = 0.0739\n",
      "step 200: mean loss = 0.0734\n",
      "step 300: mean loss = 0.0729\n",
      "step 400: mean loss = 0.0726\n",
      "step 500: mean loss = 0.0722\n",
      "step 600: mean loss = 0.0719\n",
      "step 700: mean loss = 0.0716\n",
      "step 800: mean loss = 0.0714\n",
      "step 900: mean loss = 0.0711\n"
     ]
    }
   ],
   "source": [
    "original_dim = 784\n",
    "vae = VariationalAutoEncoder(original_dim, 64, 32)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "# Iterate over epochs.\n",
    "for epoch in range(epochs):\n",
    "    print(\"Start of epoch %d\" % (epoch,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, x_batch_train in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            reconstructed = vae(x_batch_train)\n",
    "            # Compute reconstruction loss\n",
    "            loss = mse_loss_fn(x_batch_train, reconstructed)\n",
    "            loss += sum(vae.losses)  # Add KLD regularization loss\n",
    "\n",
    "        grads = tape.gradient(loss, vae.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
    "\n",
    "        loss_metric(loss)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(\"step %d: mean loss = %.4f\" % (step, loss_metric.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65e5faeb0029"
   },
   "source": [
    "Note that since the VAE is subclassing `Model`, it features built-in training\n",
    "loops. So you could also have trained it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1e98ba7ebdb8",
    "outputId": "6c6dc27e-b36d-4c9d-ef21-4263a458bb6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0748\n",
      "Epoch 2/2\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7eff08c70150>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae = VariationalAutoEncoder(784, 64, 32)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
    "vae.fit(x_train, x_train, epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42ee3169e70c"
   },
   "source": [
    "## Beyond object-oriented development: the Functional API\n",
    "\n",
    "Was this example too much object-oriented development for you? You can also\n",
    "build models using the [Functional API](https://www.tensorflow.org/guide/keras/functional/). Importantly,\n",
    "choosing one style or another does not prevent you from leveraging components\n",
    "written in the other style: you can always mix-and-match.\n",
    "\n",
    "For instance, the Functional API example below reuses the same `Sampling` layer\n",
    "we defined in the example above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b8fe39f892c7",
    "outputId": "c1359d67-d6ea-40e1-f46b-149049edcbea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0746\n",
      "Epoch 2/3\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0676\n",
      "Epoch 3/3\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7eff08c1f4d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_dim = 784\n",
    "intermediate_dim = 64\n",
    "latent_dim = 32\n",
    "\n",
    "# Define encoder model.\n",
    "original_inputs = tf.keras.Input(shape=(original_dim,), name=\"encoder_input\")\n",
    "x = layers.Dense(intermediate_dim, activation=\"relu\")(original_inputs)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()((z_mean, z_log_var))\n",
    "encoder = tf.keras.Model(inputs=original_inputs, outputs=z, name=\"encoder\")\n",
    "\n",
    "# Define decoder model.\n",
    "latent_inputs = tf.keras.Input(shape=(latent_dim,), name=\"z_sampling\")\n",
    "x = layers.Dense(intermediate_dim, activation=\"relu\")(latent_inputs)\n",
    "outputs = layers.Dense(original_dim, activation=\"sigmoid\")(x)\n",
    "decoder = tf.keras.Model(inputs=latent_inputs, outputs=outputs, name=\"decoder\")\n",
    "\n",
    "# Define VAE model.\n",
    "outputs = decoder(z)\n",
    "vae = tf.keras.Model(inputs=original_inputs, outputs=outputs, name=\"vae\")\n",
    "\n",
    "# Add KL divergence regularization loss.\n",
    "kl_loss = -0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "vae.add_loss(kl_loss)\n",
    "\n",
    "# Train.\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
    "vae.fit(x_train, x_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0f4db7df7eb5"
   },
   "source": [
    "For more information, make sure to read the [Functional API guide](https://www.tensorflow.org/guide/keras/functional/)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "custom_layers_and_models.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-gpu.2-6:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
